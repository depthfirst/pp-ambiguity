{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resolving Ambiguity in Prepositional Phrase Attachment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem of resolving ambiguity in prepositional phrase attachment is one that remains largely unsolved in NLP, and one that pre-trained language models such as BERT will likely not be of much help with. This notebook shows results of predicting prepositional phrase attachments across a subset of the NLVR2 dataset which has been annotated, leveraging a pre-trained language model commonly known as \"BERT\" (cite). \n",
    "\n",
    "We trained an SVM classifier from the output (hidden layers) of the large uncased model from BERT with whole word masking. The results are presented in terms of Cohen's kappa score and F1 score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda create -n python=3.7 ...\n",
    "# pip install transformers... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.metrics import cohen_kappa_score as kappa\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generator import HuggingFaceGenerator, MaskedPrepGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(91768)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset (train/test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = \"data\"\n",
    "outputdir = \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = json.load(open('{}/ppa_train.json'.format(datadir)))\n",
    "labels_train = [instance['label'] for instance in train_data]\n",
    "\n",
    "test_data = json.load(open('{}/ppa_test.json'.format(datadir)))\n",
    "labels_test = [instance['label'] for instance in test_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using BERT Language Model\n",
    "We load a pre-trained model from BERT and use it to generate instances for model training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_name = \"bert-large-uncased-whole-word-masking\"\n",
    "hf_generator = HuggingFaceGenerator(bert_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Dataset (or reload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature_file = \"{}/hf_train.csv\".format(outputdir)\n",
    "test_feature_file = \"{}/hf_test.csv\".format(outputdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(train_feature_file):\n",
    "    hf_train = pd.read_csv(train_feature_file, header=None)\n",
    "else:\n",
    "    hf_train = hf_generator.generate_dataset(train_data)\n",
    "    pd.DataFrame(hf_train).to_csv(train_feature_file, header=False,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(test_feature_file):\n",
    "    hf_test = pd.read_csv(test_feature_file, header=None)\n",
    "else:\n",
    "    hf_test = hf_generator.generate_dataset(test_data)\n",
    "    pd.DataFrame(hf_test).to_csv(test_feature_file, header=False,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=100.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma=0.0001, kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=91768, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfhf = svm.SVC(gamma=0.0001, C=100., random_state=91768)\n",
    "clfhf.fit(hf_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test_hf = clfhf.predict(hf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.90909091, 0.68656716, 0.5       ])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(labels_test, preds_test_hf, labels=['N','V','O'], average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6134147542598247"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kappa(labels_test, preds_test_hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'V': 36, 'N': 111, 'O': 10})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.90909091, 0.68656716])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(labels_test, preds_test_hf, labels=['N','V'], average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[105,   0,   6],\n",
       "       [  4,   4,   2],\n",
       "       [ 11,   2,  23]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(labels_test, preds_test_hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "discards = [i for i,lbl in enumerate(labels_train) if lbl=='O']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "472"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=100.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma=0.0001, kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=91768, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfhf = svm.SVC(gamma=0.0001, C=100., random_state=91768)\n",
    "clfhf.fit(hf_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_labels = [labels_test[i] for i in range(len(labels_test)) if i not in discards]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_preds = [preds_test_hf[i] for i in range(len(labels_test)) if i not in discards]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.90196078, 0.66666667])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(good_labels, good_preds, labels=['N','V'], average=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying BERT to the same dataset for the masked prep task... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpgen = MaskedPrepGenerator(bert_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(82 correct / 100 total)\n",
      "(160 correct / 200 total)\n",
      "(236 correct / 300 total)\n"
     ]
    }
   ],
   "source": [
    "labels,predictions=mpgen.evaluate_dataset(test_data,use_cuda=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7849462365591398"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(labels,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7426407001279878"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kappa(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_4tpl = lambda x : (x['V']['lemma'],x['N']['lemma'],x['P']['lemma'],x['N2']['lemma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tuples = [get_4tpl(td) for td in list(train_data)]\n",
    "test_tuples = [get_4tpl(td) for td in list(test_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = [t for i,t in enumerate(test_tuples)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'V': 36, 'N': 111, 'O': 10})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'N': 120, 'V': 31, 'O': 6})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(preds_test_hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[105,   0,   6],\n",
       "       [  4,   4,   2],\n",
       "       [ 11,   2,  23]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(labels_test,preds_test_hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = [(i,lbl,pred) for i,(lbl,pred) in enumerate(zip(labels_test,preds_test_hf)) if not lbl==pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "nvvn = [err for err in errors if 'O' not in err]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "nv = [nv for nv in nvvn if nv[1]=='N']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "vn = [vn for vn in nvvn if vn[1]=='V']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(20, 'N', 'V'),\n",
       " (57, 'N', 'V'),\n",
       " (64, 'N', 'V'),\n",
       " (67, 'N', 'V'),\n",
       " (116, 'N', 'V'),\n",
       " (145, 'N', 'V')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "nvi = [x[0] for x in nv]\n",
    "nv_data = [td for i,td in enumerate(test_data) if i in nvi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sentence_text': 'A girl in long one piece pajamas is wearing mouse ears on her head in one of the images.',\n",
       "  'label': 'N',\n",
       "  'V': {'text': 'wearing',\n",
       "   'source': 'nlvr2_dev_006.text.s.3.w.9',\n",
       "   'pos_tag': 'VBG',\n",
       "   'lemma': 'wear',\n",
       "   'trail_space': True},\n",
       "  'N': {'text': 'ears',\n",
       "   'source': 'nlvr2_dev_006.text.s.3.w.11',\n",
       "   'pos_tag': 'NNS',\n",
       "   'lemma': 'ear',\n",
       "   'trail_space': True},\n",
       "  'P': {'text': 'on',\n",
       "   'source': 'nlvr2_dev_006.text.s.3.w.12',\n",
       "   'pos_tag': 'IN',\n",
       "   'lemma': 'on',\n",
       "   'trail_space': True},\n",
       "  'N2': {'text': 'head',\n",
       "   'source': 'nlvr2_dev_006.text.s.3.w.14',\n",
       "   'pos_tag': 'NN',\n",
       "   'lemma': 'head',\n",
       "   'trail_space': True},\n",
       "  'tokenized_sentence': [['A', 'nlvr2_dev_006.text.s.3.w.1', 'DT', 'a', True],\n",
       "   ['girl', 'nlvr2_dev_006.text.s.3.w.2', 'NN', 'girl', True],\n",
       "   ['in', 'nlvr2_dev_006.text.s.3.w.3', 'IN', 'in', True],\n",
       "   ['long', 'nlvr2_dev_006.text.s.3.w.4', 'JJ', 'long', True],\n",
       "   ['one', 'nlvr2_dev_006.text.s.3.w.5', 'CD', 'one', True],\n",
       "   ['piece', 'nlvr2_dev_006.text.s.3.w.6', 'NN', 'piece', True],\n",
       "   ['pajamas', 'nlvr2_dev_006.text.s.3.w.7', 'NNS', 'pajama', True],\n",
       "   ['is', 'nlvr2_dev_006.text.s.3.w.8', 'VBZ', 'be', True],\n",
       "   ['wearing', 'nlvr2_dev_006.text.s.3.w.9', 'VBG', 'wear', True],\n",
       "   ['mouse', 'nlvr2_dev_006.text.s.3.w.10', 'NN', 'mouse', True],\n",
       "   ['ears', 'nlvr2_dev_006.text.s.3.w.11', 'NNS', 'ear', True],\n",
       "   ['on', 'nlvr2_dev_006.text.s.3.w.12', 'IN', 'on', True],\n",
       "   ['her', 'nlvr2_dev_006.text.s.3.w.13', 'PRP$', '-PRON-', True],\n",
       "   ['head', 'nlvr2_dev_006.text.s.3.w.14', 'NN', 'head', True],\n",
       "   ['in', 'nlvr2_dev_006.text.s.3.w.15', 'IN', 'in', True],\n",
       "   ['one', 'nlvr2_dev_006.text.s.3.w.16', 'CD', 'one', True],\n",
       "   ['of', 'nlvr2_dev_006.text.s.3.w.17', 'IN', 'of', True],\n",
       "   ['the', 'nlvr2_dev_006.text.s.3.w.18', 'DT', 'the', True],\n",
       "   ['images', 'nlvr2_dev_006.text.s.3.w.19', 'NNS', 'image', False],\n",
       "   ['.', 'nlvr2_dev_006.text.s.3.w.20', '.', '.', False]]},\n",
       " {'sentence_text': 'There is exactly one dog in the right image.',\n",
       "  'label': 'N',\n",
       "  'V': {'text': 'is',\n",
       "   'source': 'nlvr2_dev_013.text.s.30.w.2',\n",
       "   'pos_tag': 'VBZ',\n",
       "   'lemma': 'be',\n",
       "   'trail_space': True},\n",
       "  'N': {'text': 'dog',\n",
       "   'source': 'nlvr2_dev_013.text.s.30.w.5',\n",
       "   'pos_tag': 'NN',\n",
       "   'lemma': 'dog',\n",
       "   'trail_space': True},\n",
       "  'P': {'text': 'in',\n",
       "   'source': 'nlvr2_dev_013.text.s.30.w.6',\n",
       "   'pos_tag': 'IN',\n",
       "   'lemma': 'in',\n",
       "   'trail_space': True},\n",
       "  'N2': {'text': 'image',\n",
       "   'source': 'nlvr2_dev_013.text.s.30.w.9',\n",
       "   'pos_tag': 'NN',\n",
       "   'lemma': 'image',\n",
       "   'trail_space': False},\n",
       "  'tokenized_sentence': [['There',\n",
       "    'nlvr2_dev_013.text.s.30.w.1',\n",
       "    'EX',\n",
       "    'there',\n",
       "    True],\n",
       "   ['is', 'nlvr2_dev_013.text.s.30.w.2', 'VBZ', 'be', True],\n",
       "   ['exactly', 'nlvr2_dev_013.text.s.30.w.3', 'RB', 'exactly', True],\n",
       "   ['one', 'nlvr2_dev_013.text.s.30.w.4', 'CD', 'one', True],\n",
       "   ['dog', 'nlvr2_dev_013.text.s.30.w.5', 'NN', 'dog', True],\n",
       "   ['in', 'nlvr2_dev_013.text.s.30.w.6', 'IN', 'in', True],\n",
       "   ['the', 'nlvr2_dev_013.text.s.30.w.7', 'DT', 'the', True],\n",
       "   ['right', 'nlvr2_dev_013.text.s.30.w.8', 'JJ', 'right', True],\n",
       "   ['image', 'nlvr2_dev_013.text.s.30.w.9', 'NN', 'image', False],\n",
       "   ['.', 'nlvr2_dev_013.text.s.30.w.10', '.', '.', False]]},\n",
       " {'sentence_text': 'There is one bottle with a lid and one bottle without a lid.',\n",
       "  'label': 'N',\n",
       "  'V': {'text': 'is',\n",
       "   'source': 'nlvr2_dev_016.text.s.4.w.2',\n",
       "   'pos_tag': 'VBZ',\n",
       "   'lemma': 'be',\n",
       "   'trail_space': True},\n",
       "  'N': {'text': 'bottle',\n",
       "   'source': 'nlvr2_dev_016.text.s.4.w.4',\n",
       "   'pos_tag': 'NN',\n",
       "   'lemma': 'bottle',\n",
       "   'trail_space': True},\n",
       "  'P': {'text': 'with',\n",
       "   'source': 'nlvr2_dev_016.text.s.4.w.5',\n",
       "   'pos_tag': 'IN',\n",
       "   'lemma': 'with',\n",
       "   'trail_space': True},\n",
       "  'N2': {'text': 'lid',\n",
       "   'source': 'nlvr2_dev_016.text.s.4.w.7',\n",
       "   'pos_tag': 'NN',\n",
       "   'lemma': 'lid',\n",
       "   'trail_space': True},\n",
       "  'tokenized_sentence': [['There',\n",
       "    'nlvr2_dev_016.text.s.4.w.1',\n",
       "    'EX',\n",
       "    'there',\n",
       "    True],\n",
       "   ['is', 'nlvr2_dev_016.text.s.4.w.2', 'VBZ', 'be', True],\n",
       "   ['one', 'nlvr2_dev_016.text.s.4.w.3', 'CD', 'one', True],\n",
       "   ['bottle', 'nlvr2_dev_016.text.s.4.w.4', 'NN', 'bottle', True],\n",
       "   ['with', 'nlvr2_dev_016.text.s.4.w.5', 'IN', 'with', True],\n",
       "   ['a', 'nlvr2_dev_016.text.s.4.w.6', 'DT', 'a', True],\n",
       "   ['lid', 'nlvr2_dev_016.text.s.4.w.7', 'NN', 'lid', True],\n",
       "   ['and', 'nlvr2_dev_016.text.s.4.w.8', 'CC', 'and', True],\n",
       "   ['one', 'nlvr2_dev_016.text.s.4.w.9', 'CD', 'one', True],\n",
       "   ['bottle', 'nlvr2_dev_016.text.s.4.w.10', 'NN', 'bottle', True],\n",
       "   ['without', 'nlvr2_dev_016.text.s.4.w.11', 'IN', 'without', True],\n",
       "   ['a', 'nlvr2_dev_016.text.s.4.w.12', 'DT', 'a', True],\n",
       "   ['lid', 'nlvr2_dev_016.text.s.4.w.13', 'NN', 'lid', False],\n",
       "   ['.', 'nlvr2_dev_016.text.s.4.w.14', '.', '.', False]]},\n",
       " {'sentence_text': 'Two tall narrow cabinets have at least three upper shelves and have flat tops, but only one has two doors in its lower section.',\n",
       "  'label': 'N',\n",
       "  'V': {'text': 'has',\n",
       "   'source': 'nlvr2_dev_017.text.s.10.w.19',\n",
       "   'pos_tag': 'VBZ',\n",
       "   'lemma': 'have',\n",
       "   'trail_space': True},\n",
       "  'N': {'text': 'doors',\n",
       "   'source': 'nlvr2_dev_017.text.s.10.w.21',\n",
       "   'pos_tag': 'NNS',\n",
       "   'lemma': 'door',\n",
       "   'trail_space': True},\n",
       "  'P': {'text': 'in',\n",
       "   'source': 'nlvr2_dev_017.text.s.10.w.22',\n",
       "   'pos_tag': 'IN',\n",
       "   'lemma': 'in',\n",
       "   'trail_space': True},\n",
       "  'N2': {'text': 'section',\n",
       "   'source': 'nlvr2_dev_017.text.s.10.w.25',\n",
       "   'pos_tag': 'NN',\n",
       "   'lemma': 'section',\n",
       "   'trail_space': False},\n",
       "  'tokenized_sentence': [['Two',\n",
       "    'nlvr2_dev_017.text.s.10.w.1',\n",
       "    'CD',\n",
       "    'two',\n",
       "    True],\n",
       "   ['tall', 'nlvr2_dev_017.text.s.10.w.2', 'JJ', 'tall', True],\n",
       "   ['narrow', 'nlvr2_dev_017.text.s.10.w.3', 'JJ', 'narrow', True],\n",
       "   ['cabinets', 'nlvr2_dev_017.text.s.10.w.4', 'NNS', 'cabinet', True],\n",
       "   ['have', 'nlvr2_dev_017.text.s.10.w.5', 'VBP', 'have', True],\n",
       "   ['at', 'nlvr2_dev_017.text.s.10.w.6', 'RB', 'at', True],\n",
       "   ['least', 'nlvr2_dev_017.text.s.10.w.7', 'RBS', 'least', True],\n",
       "   ['three', 'nlvr2_dev_017.text.s.10.w.8', 'CD', 'three', True],\n",
       "   ['upper', 'nlvr2_dev_017.text.s.10.w.9', 'JJ', 'upper', True],\n",
       "   ['shelves', 'nlvr2_dev_017.text.s.10.w.10', 'NNS', 'shelf', True],\n",
       "   ['and', 'nlvr2_dev_017.text.s.10.w.11', 'CC', 'and', True],\n",
       "   ['have', 'nlvr2_dev_017.text.s.10.w.12', 'VB', 'have', True],\n",
       "   ['flat', 'nlvr2_dev_017.text.s.10.w.13', 'JJ', 'flat', True],\n",
       "   ['tops', 'nlvr2_dev_017.text.s.10.w.14', 'NNS', 'top', False],\n",
       "   [',', 'nlvr2_dev_017.text.s.10.w.15', ',', ',', True],\n",
       "   ['but', 'nlvr2_dev_017.text.s.10.w.16', 'CC', 'but', True],\n",
       "   ['only', 'nlvr2_dev_017.text.s.10.w.17', 'RB', 'only', True],\n",
       "   ['one', 'nlvr2_dev_017.text.s.10.w.18', 'CD', 'one', True],\n",
       "   ['has', 'nlvr2_dev_017.text.s.10.w.19', 'VBZ', 'have', True],\n",
       "   ['two', 'nlvr2_dev_017.text.s.10.w.20', 'CD', 'two', True],\n",
       "   ['doors', 'nlvr2_dev_017.text.s.10.w.21', 'NNS', 'door', True],\n",
       "   ['in', 'nlvr2_dev_017.text.s.10.w.22', 'IN', 'in', True],\n",
       "   ['its', 'nlvr2_dev_017.text.s.10.w.23', 'PRP$', '-PRON-', True],\n",
       "   ['lower', 'nlvr2_dev_017.text.s.10.w.24', 'JJR', 'low', True],\n",
       "   ['section', 'nlvr2_dev_017.text.s.10.w.25', 'NN', 'section', False],\n",
       "   ['.', 'nlvr2_dev_017.text.s.10.w.26', '.', '.', False]]},\n",
       " {'sentence_text': 'there are at least 3 deer in a tree eating in the image pair',\n",
       "  'label': 'N',\n",
       "  'V': {'text': 'are',\n",
       "   'source': 'nlvr2_dev_029.text.s.15.w.2',\n",
       "   'pos_tag': 'VBP',\n",
       "   'lemma': 'be',\n",
       "   'trail_space': True},\n",
       "  'N': {'text': 'deer',\n",
       "   'source': 'nlvr2_dev_029.text.s.15.w.6',\n",
       "   'pos_tag': 'NN',\n",
       "   'lemma': 'deer',\n",
       "   'trail_space': True},\n",
       "  'P': {'text': 'in',\n",
       "   'source': 'nlvr2_dev_029.text.s.15.w.7',\n",
       "   'pos_tag': 'IN',\n",
       "   'lemma': 'in',\n",
       "   'trail_space': True},\n",
       "  'N2': {'text': 'tree',\n",
       "   'source': 'nlvr2_dev_029.text.s.15.w.9',\n",
       "   'pos_tag': 'NN',\n",
       "   'lemma': 'tree',\n",
       "   'trail_space': True},\n",
       "  'tokenized_sentence': [['there',\n",
       "    'nlvr2_dev_029.text.s.15.w.1',\n",
       "    'EX',\n",
       "    'there',\n",
       "    True],\n",
       "   ['are', 'nlvr2_dev_029.text.s.15.w.2', 'VBP', 'be', True],\n",
       "   ['at', 'nlvr2_dev_029.text.s.15.w.3', 'RB', 'at', True],\n",
       "   ['least', 'nlvr2_dev_029.text.s.15.w.4', 'JJS', 'least', True],\n",
       "   ['3', 'nlvr2_dev_029.text.s.15.w.5', 'CD', '3', True],\n",
       "   ['deer', 'nlvr2_dev_029.text.s.15.w.6', 'NN', 'deer', True],\n",
       "   ['in', 'nlvr2_dev_029.text.s.15.w.7', 'IN', 'in', True],\n",
       "   ['a', 'nlvr2_dev_029.text.s.15.w.8', 'DT', 'a', True],\n",
       "   ['tree', 'nlvr2_dev_029.text.s.15.w.9', 'NN', 'tree', True],\n",
       "   ['eating', 'nlvr2_dev_029.text.s.15.w.10', 'VBG', 'eat', True],\n",
       "   ['in', 'nlvr2_dev_029.text.s.15.w.11', 'IN', 'in', True],\n",
       "   ['the', 'nlvr2_dev_029.text.s.15.w.12', 'DT', 'the', True],\n",
       "   ['image', 'nlvr2_dev_029.text.s.15.w.13', 'NN', 'image', True],\n",
       "   ['pair', 'nlvr2_dev_029.text.s.15.w.14', 'NN', 'pair', False]]},\n",
       " {'sentence_text': 'There is at least one person on the bus.',\n",
       "  'label': 'N',\n",
       "  'V': {'text': 'is',\n",
       "   'source': 'nlvr2_dev_038.text.s.19.w.2',\n",
       "   'pos_tag': 'VBZ',\n",
       "   'lemma': 'be',\n",
       "   'trail_space': True},\n",
       "  'N': {'text': 'person',\n",
       "   'source': 'nlvr2_dev_038.text.s.19.w.6',\n",
       "   'pos_tag': 'NN',\n",
       "   'lemma': 'person',\n",
       "   'trail_space': True},\n",
       "  'P': {'text': 'on',\n",
       "   'source': 'nlvr2_dev_038.text.s.19.w.7',\n",
       "   'pos_tag': 'IN',\n",
       "   'lemma': 'on',\n",
       "   'trail_space': True},\n",
       "  'N2': {'text': 'bus',\n",
       "   'source': 'nlvr2_dev_038.text.s.19.w.9',\n",
       "   'pos_tag': 'NN',\n",
       "   'lemma': 'bus',\n",
       "   'trail_space': False},\n",
       "  'tokenized_sentence': [['There',\n",
       "    'nlvr2_dev_038.text.s.19.w.1',\n",
       "    'EX',\n",
       "    'there',\n",
       "    True],\n",
       "   ['is', 'nlvr2_dev_038.text.s.19.w.2', 'VBZ', 'be', True],\n",
       "   ['at', 'nlvr2_dev_038.text.s.19.w.3', 'RB', 'at', True],\n",
       "   ['least', 'nlvr2_dev_038.text.s.19.w.4', 'RBS', 'least', True],\n",
       "   ['one', 'nlvr2_dev_038.text.s.19.w.5', 'CD', 'one', True],\n",
       "   ['person', 'nlvr2_dev_038.text.s.19.w.6', 'NN', 'person', True],\n",
       "   ['on', 'nlvr2_dev_038.text.s.19.w.7', 'IN', 'on', True],\n",
       "   ['the', 'nlvr2_dev_038.text.s.19.w.8', 'DT', 'the', True],\n",
       "   ['bus', 'nlvr2_dev_038.text.s.19.w.9', 'NN', 'bus', False],\n",
       "   ['.', 'nlvr2_dev_038.text.s.19.w.10', '.', '.', False]]}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nv_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('wear', 'ear', 'on', 'head')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_4tpl(nv_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A girl in long one piece pajamas is wearing mouse ears on her head in one of the images.\n",
      "('wear', 'ear', 'on', 'head')\n",
      "There is exactly one dog in the right image.\n",
      "('be', 'dog', 'in', 'image')\n",
      "There is one bottle with a lid and one bottle without a lid.\n",
      "('be', 'bottle', 'with', 'lid')\n",
      "Two tall narrow cabinets have at least three upper shelves and have flat tops, but only one has two doors in its lower section.\n",
      "('have', 'door', 'in', 'section')\n",
      "there are at least 3 deer in a tree eating in the image pair\n",
      "('be', 'deer', 'in', 'tree')\n",
      "There is at least one person on the bus.\n",
      "('be', 'person', 'on', 'bus')\n"
     ]
    }
   ],
   "source": [
    "for nv in nv_data:\n",
    "    print(nv['sentence_text'])\n",
    "    print(get_4tpl(nv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "vni = [x[0] for x in vn]\n",
    "vn_data = [td for i,td in enumerate(test_data) if i in vni]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are three chow dogs in the image pair.\n",
      "('be', 'dog', 'in', 'pair')\n",
      "Each dispenser has a circle shape and an upside-down raindrop shape on its front, and at least one dispenser features the raindrop shape above the circle shape.\n",
      "('feature', 'shape', 'above', 'shape')\n",
      "putting their right leg high up on a fence.\n",
      "('put', 'leg', 'on', 'fence')\n",
      "At least one of the dogs has a small toy in front of him.\n",
      "('have', 'toy', 'in', 'front')\n",
      "The combined images include an uncapped lipstick wand to the right of a capped lipstick, and a squarish smear of reddish rouge left of them.\n",
      "('include', 'wand', 'to', 'right')\n",
      "There is an awning over the machines in one of the images.\n",
      "('be', 'awning', 'over', 'machine')\n",
      "One image shows a bottle next to a white cylinder but not overlapping it, and the other image shows a single upright bottle.\n",
      "('show', 'bottle', 'to', 'cylinder')\n",
      "A shelving unit covers one wall with a unique center area, but identical sections on each side, glass upper doors in one image, and solid panels in the other.\n",
      "('cover', 'wall', 'with', 'area')\n",
      "An image shows a basset hound next to a tube shape, and the dog has at least one front paw propped over something.\n",
      "('show', 'hound', 'to', 'shape')\n",
      "There is a dog on a green rug.\n",
      "('be', 'dog', 'on', 'rug')\n",
      "There are exactly two ducks in the right image.\n",
      "('be', 'duck', 'in', 'image')\n"
     ]
    }
   ],
   "source": [
    "for vn in vn_data:\n",
    "    print(vn['sentence_text'])\n",
    "    print(get_4tpl(vn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpls = [get_4tpl(td) for td in train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = [(t[0],t[2]) for t in tpls] + [(t[1],t[2]) for t in tpls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=Counter(bigrams).most_common(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_tpls = [tuple((*get_4tpl(td),td['label'])) for td in train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_bigrams = [(t[0],t[2],t[4]=='V') for t in labeled_tpls] + [(t[1],t[2],t[4]=='N') for t in labeled_tpls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('be', 'in', True): 51, ('be', 'in', False): 9})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([b for b in labeled_bigrams if b[0]=='be' and b[1]=='in'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('be', 'in'), 60): 51/60=0.850\n",
      "(('show', 'in'), 18): 2/18=0.111\n",
      "(('dog', 'in'), 14): 2/14=0.143\n",
      "(('have', 'in'), 10): 6/10=0.600\n",
      "(('be', 'on'), 9): 7/9=0.778\n",
      "(('have', 'on'), 7): 3/7=0.429\n",
      "(('feature', 'on'), 6): 1/6=0.167\n",
      "(('show', 'on'), 6): 1/6=0.167\n"
     ]
    }
   ],
   "source": [
    "for ctr in c:\n",
    "    matches = [lb[2] for lb in labeled_bigrams if (lb[0],lb[1])==(ctr[0][0],ctr[0][1])]\n",
    "    if len(matches)==0:\n",
    "        print(\"Not found: {}\".format((ctr[0][0],ctr[0][1])))\n",
    "    num_pos = len([m for m in matches if m])\n",
    "    proportion = float(num_pos/len(matches))\n",
    "    if proportion>0.1 and proportion<0.9:\n",
    "        print(\"{}: {}/{}={:.3f}\".format(ctr,num_pos,len(matches),proportion))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These would seem to be the cases that are not clearly discriminated. \n",
    "Unless these are attributable to annotation errors, we may need additional context to distinguish these cases. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recover the context for these cases... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('be', 'skunk', 'in', 'total', 'V'),\n",
       " ('be', 'filling', 'in', 'bread', 'N'),\n",
       " ('be', 'animal', 'in', 'total', 'V'),\n",
       " ('be', 'dog', 'in', 'pair', 'V'),\n",
       " ('be', 'man', 'in', 'sleeve', 'N'),\n",
       " ('be', 'product', 'in', 'image', 'V'),\n",
       " ('be', 'dog', 'in', 'total', 'V'),\n",
       " ('be', 'kid', 'in', 'pair', 'V'),\n",
       " ('be', 'dog', 'in', 'image', 'V'),\n",
       " ('be', 'crab', 'in', 'image', 'V'),\n",
       " ('be', 'stingray', 'in', 'pair', 'V'),\n",
       " ('be', 'cup', 'in', 'image', 'V'),\n",
       " ('be', 'mammal', 'in', 'image', 'N'),\n",
       " ('be', 'bottle', 'in', 'image', 'V'),\n",
       " ('be', 'pelican', 'in', 'image', 'V'),\n",
       " ('be', 'person', 'in', 'library', 'V'),\n",
       " ('be', 'ibex', 'in', 'image', 'V'),\n",
       " ('be', 'dog', 'in', 'image', 'V'),\n",
       " ('be', 'horse', 'in', 'image', 'V'),\n",
       " ('be', 'dog', 'in', 'image', 'N'),\n",
       " ('be', 'panda', 'in', 'image', 'V'),\n",
       " ('be', 'bottle', 'in', 'total', 'V'),\n",
       " ('be', 'item', 'in', 'image', 'V'),\n",
       " ('be', 'saxophone', 'in', 'total', 'V'),\n",
       " ('be', 'bottle', 'in', 'total', 'V'),\n",
       " ('be', 'female', 'in', 'bikini', 'N'),\n",
       " ('be', 'dessert', 'in', 'image', 'V'),\n",
       " ('be', 'wall', 'in', 'background', 'V'),\n",
       " ('be', 'bottle', 'in', 'image', 'V'),\n",
       " ('be', 'parrot', 'in', 'image', 'V'),\n",
       " ('be', 'animal', 'in', 'image', 'V'),\n",
       " ('be', 'jellyfish', 'in', 'total', 'V'),\n",
       " ('be', 'dog', 'in', 'image', 'V'),\n",
       " ('be', 'creature', 'in', 'image', 'V'),\n",
       " ('be', 'human', 'in', 'image', 'V'),\n",
       " ('be', 'nothing', 'in', 'shelf', 'V'),\n",
       " ('be', 'cup', 'in', 'image', 'V'),\n",
       " ('be', 'bird', 'in', 'image', 'V'),\n",
       " ('be', 'monitor', 'in', 'image', 'V'),\n",
       " ('be', 'saxophone', 'in', 'image', 'V'),\n",
       " ('be', 'cover', 'in', 'stripe', 'N'),\n",
       " ('be', 'bird', 'in', 'pair', 'V'),\n",
       " ('be', 'snow', 'in', 'truck', 'V'),\n",
       " ('be', 'woman', 'in', 'gown', 'N'),\n",
       " ('be', 'seal', 'in', 'pair', 'V'),\n",
       " ('be', 'dog', 'in', 'image', 'V'),\n",
       " ('be', 'dog', 'in', 'image', 'V'),\n",
       " ('be', 'roll', 'in', 'image', 'V'),\n",
       " ('be', 'crab', 'in', 'image', 'V'),\n",
       " ('be', 'people', 'in', 'image', 'V'),\n",
       " ('be', 'gorilla', 'in', 'image', 'V'),\n",
       " ('be', 'dog', 'in', 'image', 'V'),\n",
       " ('be', 'door', 'in', 'image', 'V'),\n",
       " ('be', 'penguin', 'in', 'water', 'V'),\n",
       " ('be', 'dog', 'in', 'total', 'V'),\n",
       " ('be', 'dispenser', 'in', 'total', 'V'),\n",
       " ('be', 'teacup', 'in', 'image', 'V'),\n",
       " ('be', 'jellyfish', 'in', 'water', 'N'),\n",
       " ('be', 'plug', 'in', 'basin', 'N'),\n",
       " ('be', 'chihuahua', 'in', 'image', 'V')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[t for t in labeled_tpls if t[0]=='be' and t[2]=='in']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('be', 'filling', 'in', 'bread', 'N'),\n",
       " ('be', 'man', 'in', 'sleeve', 'N'),\n",
       " ('be', 'mammal', 'in', 'image', 'N'),\n",
       " ('be', 'dog', 'in', 'image', 'N'),\n",
       " ('be', 'female', 'in', 'bikini', 'N'),\n",
       " ('be', 'cover', 'in', 'stripe', 'N'),\n",
       " ('be', 'woman', 'in', 'gown', 'N'),\n",
       " ('be', 'jellyfish', 'in', 'water', 'N'),\n",
       " ('be', 'plug', 'in', 'basin', 'N')]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[t for t in labeled_tpls if t[0]=='be' and t[2]=='in' and t[4]=='N']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm not too sure about some of these. We generally expect 'in image' to attach to the verb. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data format\n",
    "### -----------\n",
    "There are train and test files, identified by different prefixes. Each line in the the files is a PP attachment instance, in the following format:\n",
    "```\n",
    "<prefix>.preps.words - the preposition.\n",
    "<prefix>.children.words - the preposition's child.\n",
    "<prefix>.heads.words - the candidate heads for the PP.\n",
    "<prefix>.heads.pos - the part-of-speech of the candidate heads for the PP, where \"1\" is for verbs and \"-1\" is for nouns.\n",
    "<prefix>.heads.next.pos - the part-of-speech of the words following the candidate heads of the PP, in text format.\n",
    "<prefix>.labels - the gold head of the PP, indicated by an index specifying its position in the list of candidate heads (e.g. in <prefix>.heads.words)\n",
    "<prefix>.nheads - the number of candidate heads. \n",
    "```\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "nheads = pd.read_csv('data/wsj.2-21.txt.dep.pp.nheads',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "heads = pd.read_csv('data/wsj.2-21.txt.dep.pp.heads.words',header=None)[nheads[0]==2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "preps = pd.read_csv('data/wsj.2-21.txt.dep.pp.preps.words',header=None)[nheads[0]==2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv('data/wsj.2-21.txt.dep.pp.labels',header=None)[nheads[0]==2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = pd.read_csv('data/wsj.2-21.txt.dep.pp.heads.pos',header=None)[nheads[0]==2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "nheadslist = nheads[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "headcandposlist=[tuple(hh.split(\"\\t\")) for hh in pos[0].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "vnlist = [i for i,hc in enumerate(headcandposlist) if hc[0]>hc[1]] # verb-noun candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "headslist = pd.read_csv('data/wsj.2-21.txt.dep.pp.heads.words',header=None)[nheads[0]==2][0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "vnheads = [headslist[i] for i in vnlist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "vnverbs = [vn.split()[0] for vn in vnheads]\n",
    "vnnouns = [vn.split()[1] for vn in vnheads]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepslist = preps[0].tolist()\n",
    "vnpreps = [prepslist[i] for i in vnlist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelslist = labels[0].tolist()\n",
    "relabel = lambda x : 'V' if int(x)==1 else 'N'\n",
    "vnlabels = [relabel(labelslist[i]) for i in vnlist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "children = pd.read_csv('data/wsj.2-21.txt.dep.pp.children.words',header=None)[nheads[0]==2][0].tolist()\n",
    "vnchildren = [children[i] for i in vnlist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsj_labeled_tuples = [x for x in zip(vnverbs,vnnouns,vnpreps,vnchildren,vnlabels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('be', 'charge', 'of', 'research', 'N')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wsj_labeled_tuples[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('be', 'volume', 'in', 'stocks', 'N'), ('be', 'wind', 'in', 'sails', 'N')]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[t for t in wsj_labeled_tuples if t[0]=='be' and t[2]=='in' and t[4]=='N']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_wsj_bigrams = [(t[0],t[2],t[4]=='V') for t in wsj_labeled_tuples] + [(t[1],t[2],t[4]=='N') for t in wsj_labeled_tuples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsj_bigrams = [(t[0],t[2]) for t in wsj_labeled_tuples] + [(t[1],t[2]) for t in wsj_labeled_tuples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "c2=Counter(wsj_bigrams).most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('be', 'in'), 60): 2/4=0.500\n",
      "Not found: ('contain', 'of')\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-9b0071b34386>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Not found: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mctr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mnum_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmatches\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mproportion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_pos\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mproportion\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0.1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mproportion\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}: {}/{}={:.3f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_pos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mproportion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "for ctr in c:\n",
    "    matches = [lb[2] for lb in labeled_wsj_bigrams if (lb[0],lb[1])==(ctr[0][0],ctr[0][1])]\n",
    "    if len(matches)==0:\n",
    "        print(\"Not found: {}\".format((ctr[0][0],ctr[0][1])))\n",
    "    num_pos = len([m for m in matches if m])\n",
    "    proportion = float(num_pos/len(matches))\n",
    "    if proportion>0.1 and proportion<0.9:\n",
    "        print(\"{}: {}/{}={:.3f}\".format(ctr,num_pos,len(matches),proportion))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem: this dataset does not use lemma, but that's what I've been doing. \n",
    "I can either get the lemma for the new dataset or go back and get the words for my dataset.  \n",
    "Since the former may introduce another source of error, I'll try the latter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
