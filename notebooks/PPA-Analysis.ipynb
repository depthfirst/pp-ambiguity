{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resolving Ambiguity in Prepositional Phrase Attachment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem of resolving ambiguity in prepositional phrase attachment is one that remains largely unsolved in NLP, and one that pre-trained language models such as BERT will likely not be of much help with. This notebook shows results of predicting prepositional phrase attachments across a subset of the NLVR2 dataset which has been annotated, leveraging a pre-trained language model commonly known as \"BERT\" (cite). \n",
    "\n",
    "We trained an SVM classifier from the output (hidden layers) of the large uncased model from BERT with whole word masking. The results are presented in terms of Cohen's kappa score and F1 score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda create -n python=3.7 ...\n",
    "# pip install transformers... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.metrics import cohen_kappa_score as kappa\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "from collections import Counter\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generator import HuggingFaceGenerator, MaskedPrepGenerator, SpacyModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(91768)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset (train/test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = \"data\"\n",
    "outputdir = \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = json.load(open('{}/ppa_train.json'.format(datadir)))\n",
    "labels_train = [instance['label'] for instance in train_data]\n",
    "\n",
    "test_data = json.load(open('{}/ppa_test.json'.format(datadir)))\n",
    "labels_test = [instance['label'] for instance in test_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using BERT Language Model\n",
    "We load a pre-trained model from BERT and use it to generate instances for model training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_name = \"bert-large-uncased-whole-word-masking\"\n",
    "hf_generator = HuggingFaceGenerator(bert_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Dataset (or reload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature_file = \"{}/hf_train.csv\".format(outputdir)\n",
    "test_feature_file = \"{}/hf_test.csv\".format(outputdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(train_feature_file):\n",
    "    hf_train = pd.read_csv(train_feature_file, header=None)\n",
    "else:\n",
    "    hf_train = hf_generator.generate_dataset(train_data)\n",
    "    pd.DataFrame(hf_train).to_csv(train_feature_file, header=False,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(test_feature_file):\n",
    "    hf_test = pd.read_csv(test_feature_file, header=None)\n",
    "else:\n",
    "    hf_test = hf_generator.generate_dataset(test_data)\n",
    "    pd.DataFrame(hf_test).to_csv(test_feature_file, header=False,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=100.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma=0.0001, kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=91768, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfhf = svm.SVC(gamma=0.0001, C=100., random_state=91768)\n",
    "clfhf.fit(hf_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test_hf = clfhf.predict(hf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N      0.875     0.946     0.909       111\n",
      "           O      0.667     0.400     0.500        10\n",
      "           V      0.742     0.639     0.687        36\n",
      "\n",
      "    accuracy                          0.841       157\n",
      "   macro avg      0.761     0.662     0.699       157\n",
      "weighted avg      0.831     0.841     0.832       157\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.metrics.classification_report(labels_test, preds_test_hf, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### without 'O'(ther) classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nvi = [i for i,lbl in enumerate(labels_train) if lbl in ['N','V']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train_NV = [lbl for lbl in labels_train if lbl in ['N','V']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_train_NV = hf_train.to_numpy()[nvi]\n",
    "#hf_generator.generate_dataset([td for td,lbl in zip(train_data,labels_train) if lbl in ['N','V']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(435, 16384)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_train_NV.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nvi_test = [i for i,lbl in enumerate(labels_test) if lbl in ['N','V']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_test_NV = hf_test.to_numpy()[nvi_test]\n",
    "#hf_generator.generate_dataset([td for td,lbl in zip(test_data,labels_test) if lbl in ['N','V']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_test_NV = [lbl for lbl in labels_test if lbl in ['N','V']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=100.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma=0.0001, kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=91768, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfhf2 = svm.SVC(gamma=0.0001, C=100., random_state=91768)\n",
    "clfhf2.fit(hf_train_NV, labels_train_NV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test_hf2 = clfhf2.predict(hf_test_NV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[105,   0,   6],\n",
       "       [  4,   4,   2],\n",
       "       [ 11,   2,  23]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(labels_test, preds_test_hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[104,   7],\n",
       "       [ 12,  24]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(labels_test_NV, preds_test_hf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N      0.897     0.937     0.916       111\n",
      "           V      0.774     0.667     0.716        36\n",
      "\n",
      "    accuracy                          0.871       147\n",
      "   macro avg      0.835     0.802     0.816       147\n",
      "weighted avg      0.867     0.871     0.867       147\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.metrics.classification_report(labels_test_NV, preds_test_hf2, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_4tpl = lambda x : (x['V']['lemma'],x['N']['lemma'],x['P']['lemma'],x['N2']['lemma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_model = SpacyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_preds_train = [spacy_model.predict(t) for t in train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tuples = [get_4tpl(td) for td in list(train_data)]\n",
    "test_tuples = [get_4tpl(td) for td in list(test_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = [(i,lbl,pred) for i,(lbl,pred) in enumerate(zip(labels_test,preds_test_hf)) if not lbl==pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_err_info(err,data):\n",
    "    (i,lbl,pred)=err\n",
    "    td=data[i]\n",
    "    #print(td['sentence_text'])\n",
    "    return ((i,*get_4tpl(td),lbl,pred,td['sentence_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_tuples=[]\n",
    "for err in errors: \n",
    "    # Exclude 'other' cases for now\n",
    "    if 'O' in err:\n",
    "        continue\n",
    "    error_tuples.append(get_err_info(err,test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(error_tuples, columns=['i','V','N1','P','N2','label','pred','sentence_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i</th>\n",
       "      <th>V</th>\n",
       "      <th>N1</th>\n",
       "      <th>P</th>\n",
       "      <th>N2</th>\n",
       "      <th>label</th>\n",
       "      <th>pred</th>\n",
       "      <th>sentence_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>be</td>\n",
       "      <td>dog</td>\n",
       "      <td>in</td>\n",
       "      <td>pair</td>\n",
       "      <td>V</td>\n",
       "      <td>N</td>\n",
       "      <td>There are three chow dogs in the image pair.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>wear</td>\n",
       "      <td>ear</td>\n",
       "      <td>on</td>\n",
       "      <td>head</td>\n",
       "      <td>N</td>\n",
       "      <td>V</td>\n",
       "      <td>A girl in long one piece pajamas is wearing mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>feature</td>\n",
       "      <td>shape</td>\n",
       "      <td>above</td>\n",
       "      <td>shape</td>\n",
       "      <td>V</td>\n",
       "      <td>N</td>\n",
       "      <td>Each dispenser has a circle shape and an upsid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>57</td>\n",
       "      <td>be</td>\n",
       "      <td>dog</td>\n",
       "      <td>in</td>\n",
       "      <td>image</td>\n",
       "      <td>N</td>\n",
       "      <td>V</td>\n",
       "      <td>There is exactly one dog in the right image.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>64</td>\n",
       "      <td>be</td>\n",
       "      <td>bottle</td>\n",
       "      <td>with</td>\n",
       "      <td>lid</td>\n",
       "      <td>N</td>\n",
       "      <td>V</td>\n",
       "      <td>There is one bottle with a lid and one bottle ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>67</td>\n",
       "      <td>have</td>\n",
       "      <td>door</td>\n",
       "      <td>in</td>\n",
       "      <td>section</td>\n",
       "      <td>N</td>\n",
       "      <td>V</td>\n",
       "      <td>Two tall narrow cabinets have at least three u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>72</td>\n",
       "      <td>put</td>\n",
       "      <td>leg</td>\n",
       "      <td>on</td>\n",
       "      <td>fence</td>\n",
       "      <td>V</td>\n",
       "      <td>N</td>\n",
       "      <td>putting their right leg high up on a fence.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>75</td>\n",
       "      <td>have</td>\n",
       "      <td>toy</td>\n",
       "      <td>in</td>\n",
       "      <td>front</td>\n",
       "      <td>V</td>\n",
       "      <td>N</td>\n",
       "      <td>At least one of the dogs has a small toy in fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>78</td>\n",
       "      <td>include</td>\n",
       "      <td>wand</td>\n",
       "      <td>to</td>\n",
       "      <td>right</td>\n",
       "      <td>V</td>\n",
       "      <td>N</td>\n",
       "      <td>The combined images include an uncapped lipsti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>83</td>\n",
       "      <td>be</td>\n",
       "      <td>awning</td>\n",
       "      <td>over</td>\n",
       "      <td>machine</td>\n",
       "      <td>V</td>\n",
       "      <td>N</td>\n",
       "      <td>There is an awning over the machines in one of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>105</td>\n",
       "      <td>show</td>\n",
       "      <td>bottle</td>\n",
       "      <td>to</td>\n",
       "      <td>cylinder</td>\n",
       "      <td>V</td>\n",
       "      <td>N</td>\n",
       "      <td>One image shows a bottle next to a white cylin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>112</td>\n",
       "      <td>cover</td>\n",
       "      <td>wall</td>\n",
       "      <td>with</td>\n",
       "      <td>area</td>\n",
       "      <td>V</td>\n",
       "      <td>N</td>\n",
       "      <td>A shelving unit covers one wall with a unique ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>116</td>\n",
       "      <td>be</td>\n",
       "      <td>deer</td>\n",
       "      <td>in</td>\n",
       "      <td>tree</td>\n",
       "      <td>N</td>\n",
       "      <td>V</td>\n",
       "      <td>there are at least 3 deer in a tree eating in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>123</td>\n",
       "      <td>show</td>\n",
       "      <td>hound</td>\n",
       "      <td>to</td>\n",
       "      <td>shape</td>\n",
       "      <td>V</td>\n",
       "      <td>N</td>\n",
       "      <td>An image shows a basset hound next to a tube s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>142</td>\n",
       "      <td>be</td>\n",
       "      <td>dog</td>\n",
       "      <td>on</td>\n",
       "      <td>rug</td>\n",
       "      <td>V</td>\n",
       "      <td>N</td>\n",
       "      <td>There is a dog on a green rug.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>145</td>\n",
       "      <td>be</td>\n",
       "      <td>person</td>\n",
       "      <td>on</td>\n",
       "      <td>bus</td>\n",
       "      <td>N</td>\n",
       "      <td>V</td>\n",
       "      <td>There is at least one person on the bus.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>154</td>\n",
       "      <td>be</td>\n",
       "      <td>duck</td>\n",
       "      <td>in</td>\n",
       "      <td>image</td>\n",
       "      <td>V</td>\n",
       "      <td>N</td>\n",
       "      <td>There are exactly two ducks in the right image.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      i        V      N1      P        N2 label pred  \\\n",
       "0     0       be     dog     in      pair     V    N   \n",
       "1    20     wear     ear     on      head     N    V   \n",
       "2    28  feature   shape  above     shape     V    N   \n",
       "3    57       be     dog     in     image     N    V   \n",
       "4    64       be  bottle   with       lid     N    V   \n",
       "5    67     have    door     in   section     N    V   \n",
       "6    72      put     leg     on     fence     V    N   \n",
       "7    75     have     toy     in     front     V    N   \n",
       "8    78  include    wand     to     right     V    N   \n",
       "9    83       be  awning   over   machine     V    N   \n",
       "10  105     show  bottle     to  cylinder     V    N   \n",
       "11  112    cover    wall   with      area     V    N   \n",
       "12  116       be    deer     in      tree     N    V   \n",
       "13  123     show   hound     to     shape     V    N   \n",
       "14  142       be     dog     on       rug     V    N   \n",
       "15  145       be  person     on       bus     N    V   \n",
       "16  154       be    duck     in     image     V    N   \n",
       "\n",
       "                                        sentence_text  \n",
       "0        There are three chow dogs in the image pair.  \n",
       "1   A girl in long one piece pajamas is wearing mo...  \n",
       "2   Each dispenser has a circle shape and an upsid...  \n",
       "3        There is exactly one dog in the right image.  \n",
       "4   There is one bottle with a lid and one bottle ...  \n",
       "5   Two tall narrow cabinets have at least three u...  \n",
       "6         putting their right leg high up on a fence.  \n",
       "7   At least one of the dogs has a small toy in fr...  \n",
       "8   The combined images include an uncapped lipsti...  \n",
       "9   There is an awning over the machines in one of...  \n",
       "10  One image shows a bottle next to a white cylin...  \n",
       "11  A shelving unit covers one wall with a unique ...  \n",
       "12  there are at least 3 deer in a tree eating in ...  \n",
       "13  An image shows a basset hound next to a tube s...  \n",
       "14                     There is a dog on a green rug.  \n",
       "15           There is at least one person on the bus.  \n",
       "16    There are exactly two ducks in the right image.  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not sure:\n",
    "# There are at least 3 deer in a tree...\n",
    "# There is a dog on a green rug. \n",
    "# also not sure what to do about 'next to' => compound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some instances seem to be labeled incorrectly\n",
    "wrong_label_indices = [1,3,12,15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test_labels = [lbl for lbl in labels_test]\n",
    "for i in df.to_numpy()[wrong_label_indices,0]:\n",
    "    new_test_labels[i] = 'V'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze train/dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_preds = [spacy_model.predict(t) for t in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = ['all_noun','spacy','bert','spacy+pvc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = [['N' for i in range(len(labels_test))], spacy_preds, preds_test_hf,corrected_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_reports = {cl: sklearn.metrics.classification_report(labels_test, preds, digits=3, output_dict=True, zero_division=0) \n",
    "                 for (cl,preds) in zip(classifiers, test_preds)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_reports = {cl: sklearn.metrics.classification_report(labels_test, preds, digits=3, output_dict=True, zero_division=0) \n",
    "                 for (cl,preds) in zip(classifiers, test_preds)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>weighted avg f1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>all_noun</td>\n",
       "      <td>0.585655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spacy</td>\n",
       "      <td>0.722477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bert</td>\n",
       "      <td>0.832010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spacy+pvc</td>\n",
       "      <td>0.761087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  classifier  weighted avg f1-score\n",
       "0   all_noun               0.585655\n",
       "1      spacy               0.722477\n",
       "2       bert               0.832010\n",
       "3  spacy+pvc               0.761087"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([[c, class_reports[c]['weighted avg']['f1-score']] for c in classifiers], columns=['classifier','weighted avg f1-score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_reports = {cl: sklearn.metrics.classification_report(corrected_labels, preds, digits=3, output_dict=True, zero_division=0) \n",
    "                 for (cl,preds) in zip(classifiers, test_preds)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>weighted avg f1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>all_noun</td>\n",
       "      <td>0.440713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spacy</td>\n",
       "      <td>0.594235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bert</td>\n",
       "      <td>0.750900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spacy+pvc</td>\n",
       "      <td>0.873773</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  classifier  weighted avg f1-score\n",
       "0   all_noun               0.440713\n",
       "1      spacy               0.594235\n",
       "2       bert               0.750900\n",
       "3  spacy+pvc               0.873773"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([[c, class_reports[c]['weighted avg']['f1-score']] for c in classifiers], columns=['classifier','weighted avg f1-score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subtuples(tpl):\n",
    "    triples = [tpl[:3],(tpl[0],*tpl[2:]),tpl[1:]]\n",
    "    doubles = [(tpl[0],tpl[2]),tpl[1:2],tpl[2:]]\n",
    "    singles = [tuple([t]) for t in tpl]\n",
    "    subtuples = [(tpl)] + triples + doubles + singles\n",
    "    return subtuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbs_df = pd.DataFrame.from_dict(Counter([v for (v,n1,p,n2) in train_tuples]),orient='index') \\\n",
    "    .reset_index() \\\n",
    "    .rename(columns={\"index\":\"verb\",0:\"count\"}) \\\n",
    "    .sort_values(by='count', ascending=False, ignore_index=True, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>verb</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>be</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>show</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>contain</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>feature</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>have</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>plow</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>cap</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>make</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>bear</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>grasp</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       verb  count\n",
       "0        be    117\n",
       "1      show     91\n",
       "2   contain     72\n",
       "3   feature     32\n",
       "4      have     29\n",
       "..      ...    ...\n",
       "63     plow      1\n",
       "64      cap      1\n",
       "65     make      1\n",
       "66     bear      1\n",
       "67    grasp      1\n",
       "\n",
       "[68 rows x 2 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verbs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "presentation_verb_lemmas = ['include','feature','show','be',\n",
    "                            'wear','cover','contain','have']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "presentation_verb_test = verbs_df['verb'].map(lambda x: x in presentation_verb_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonpresentation_verb_test = verbs_df['verb'].map(lambda x: x not in presentation_verb_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Presentation verbs or not? \n",
    "s=\"hold, face, see, match, depict, look, view, stare, angle, expose, draw, receive, frame, accompany, wave\"\n",
    "pres_candidates = s.split(\", \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What proportion of training data feature\n",
    "presentation verbs? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    0.800847\n",
       "dtype: float64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verbs_df[presentation_verb_test].sum(axis=0,numeric_only=True)/len(train_tuples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Presentation verbs identified comprise 80% of the training data.\n",
    " Another 8% or so seem like they might or might not belong to this category.\n",
    " The rest (12-20% of the data) are not in this category and would thus attach to the noun. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "pres_verb_df = pd.DataFrame([\n",
    "    [*td,lbl]\n",
    "    for td,lbl in zip(train_tuples,labels_train)\n",
    "    if td[0] in presentation_verb_lemmas\n",
    "],columns=[\"verb\",\"noun\",\"prep\",\"pobj\",\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'N': 277, 'V': 82, 'O': 19})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(pres_verb_df.label.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>verb</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>be</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>show</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>contain</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>feature</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>have</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>include</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>wear</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cover</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      verb  count\n",
       "0       be    117\n",
       "1     show     91\n",
       "2  contain     72\n",
       "3  feature     32\n",
       "4     have     29\n",
       "5  include     18\n",
       "6     wear     17\n",
       "7    cover      2"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(Counter(pres_verb_df.verb.tolist()),orient=\"index\") \\\n",
    "    .reset_index() \\\n",
    "    .rename(columns={\"index\":\"verb\",0:\"count\"}) \\\n",
    "    .sort_values(by='count', ascending=False, ignore_index=True, inplace=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wanted to review all the cases with presentation verbs to see which ones describe a state that changes over time. I flagged them all with \"Y\", \"N\", or \"?\", accordingly (\"?\" indicates uncertainty or need to see the rest of sentence). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "pres_verb_df.to_csv('data/presentation_verbs.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('data/presentation_verbs_with_state.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Want PP+POBJ for all cases with mod_state_mutable=Y\n",
    "matches=list(df[df['mod_state_mutable']=='Y'].itertuples(index=False,name=None))\n",
    "pp_matches = set([(x[2],x[3]) for x in matches])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this as a starting point, we can correct some of the parses from spaCy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E: ('form', 'screen', 'in', 'front')\n",
      "E: ('hold', 'dog', 'of', 'image')\n",
      "E: ('leave', 'pig', 'in', 'image')\n",
      "E: ('show', 'hound', 'on', 'grass')\n",
      "E: ('have', 'hole', 'in', 'front')\n",
      "E: ('be', 'pillow', 'of', 'image')\n",
      "E: ('show', 'cover', 'on', 'side')\n",
      "E: ('feature', 'oval', 'on', 'image')\n",
      "E: ('view', 'head', 'on', 'left')\n",
      "E: ('surround', 'table', 'in', 'image')\n",
      "E: ('flesh', 'fruit', 'on', 'top')\n",
      "E: ('contain', 'gorilla', 'in', 'image')\n",
      "E: ('wear', 'pack', 'in', 'image')\n",
      "E: ('feature', 'dog', 'on', 'left')\n",
      "E: ('mash', 'potato', 'in', 'bowl')\n",
      "E: ('be', 'dog', 'on', 'right')\n",
      "E: ('show', 'person', 'in', 'sleeve')\n",
      "E: ('show', 'skunk', 'in', 'profile')\n",
      "E: ('leave', 'display', 'in', 'image')\n",
      "E: ('be', 'spoon', 'on', 'top')\n",
      "E: ('show', 'buffalo', 'in', 'water')\n"
     ]
    }
   ],
   "source": [
    "corrected_preds = []\n",
    "ann_vs_predV = []\n",
    "for i,(tt,lbl,spc) in enumerate(zip(test_tuples, new_test_labels, spacy_preds)):\n",
    "    pp = (tt[2],tt[3])\n",
    "    \n",
    "    if pp in pp_matches:\n",
    "        if lbl!='V':\n",
    "            print(\"E:\",tt)\n",
    "            ann_vs_predV.append(tt)\n",
    "        corrected_preds.append('V')\n",
    "    else:\n",
    "        corrected_preds.append(spc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_df = pd.DataFrame(ann_vs_predV,columns=['verb','noun','prep','pobj'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a table of this, show it... \n",
    "# Flag cases I'm still unsure about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrong === lbl=N/O, pred=V, but lbl should be V\n",
    "wrong = [0,0,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]\n",
    "flagged = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_df['wrong'] = wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>verb</th>\n",
       "      <th>noun</th>\n",
       "      <th>prep</th>\n",
       "      <th>pobj</th>\n",
       "      <th>wrong</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>form</td>\n",
       "      <td>screen</td>\n",
       "      <td>in</td>\n",
       "      <td>front</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hold</td>\n",
       "      <td>dog</td>\n",
       "      <td>of</td>\n",
       "      <td>image</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>have</td>\n",
       "      <td>hole</td>\n",
       "      <td>in</td>\n",
       "      <td>front</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>be</td>\n",
       "      <td>pillow</td>\n",
       "      <td>of</td>\n",
       "      <td>image</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   verb    noun prep   pobj  wrong\n",
       "0  form  screen   in  front      0\n",
       "1  hold     dog   of  image      0\n",
       "4  have    hole   in  front      0\n",
       "5    be  pillow   of  image      0"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_df[err_df['wrong']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_labels_test=err_df[err_df['wrong']==1][['verb','noun','prep','pobj']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrongly_labeled_tpls = list(wrong_labels_test.to_records(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_labels = []\n",
    "for tpl,lbl in zip(test_tuples, new_test_labels):\n",
    "    corrected = False\n",
    "    for wtpl in wrongly_labeled_tpls:\n",
    "        if tuple(tpl)==tuple(wtpl):\n",
    "            corrected_labels.append('V')\n",
    "            corrected = True\n",
    "            continue\n",
    "    if not corrected:\n",
    "        corrected_labels.append(lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N      0.916     0.784     0.845       111\n",
      "           O      0.375     0.300     0.333        10\n",
      "           V      0.519     0.778     0.622        36\n",
      "\n",
      "    accuracy                          0.752       157\n",
      "   macro avg      0.603     0.621     0.600       157\n",
      "weighted avg      0.790     0.752     0.761       157\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.metrics.classification_report(labels_test, corrected_preds, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disappointing, but I think error analysis will show\n",
    "# that the annotations are wrong, or at least questionable\n",
    "# ...show test cases where annotation and correction disagree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N      0.905     0.925     0.915        93\n",
      "           O      0.375     0.429     0.400         7\n",
      "           V      0.889     0.842     0.865        57\n",
      "\n",
      "    accuracy                          0.873       157\n",
      "   macro avg      0.723     0.732     0.727       157\n",
      "weighted avg      0.876     0.873     0.874       157\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.metrics.classification_report(corrected_labels, corrected_preds, digits=3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
