{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resolving Ambiguity in Prepositional Phrase Attachment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows results of predicting prepositional phrase attachments across a subset of the NLVR2 dataset which has been annotated. \n",
    "\n",
    "The first group of models are trained from the output the large uncased model from BERT with whole word masking. \n",
    "This model was subsequently converted to PyTorch/HuggingFace via command-line. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blah blah blah about prepositional phrase attachments... \n",
    "\n",
    "Blah blah blah some interesting examples. \n",
    "\n",
    "Blah blah blah about NLVR2 paper and dataset\n",
    "\n",
    "Some stuff about this dataset and how it was collected\n",
    "and how it was annotated\n",
    "\n",
    "What this notebook shows... \n",
    "\n",
    "(my sig)\n",
    "\n",
    "Prelims\n",
    "Imports\n",
    "outline/toc\n",
    "Background\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda create -n python=3.7 ...\n",
    "# pip install transformers... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import torch\n",
    "import spacy\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn.metrics import cohen_kappa_score as kappa\n",
    "from itertools import groupby\n",
    "\n",
    "from sklearn import svm\n",
    "from collections import Counter\n",
    "\n",
    "#sys.path.append('/bridge/science/AI/nlp/bert')\n",
    "#from notebook_source import generate_huggingface_instances\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bridge/science/laboratory/conda/envs/spacy/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/bridge/science/laboratory/conda/envs/spacy/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/bridge/science/laboratory/conda/envs/spacy/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/bridge/science/laboratory/conda/envs/spacy/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/bridge/science/laboratory/conda/envs/spacy/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/bridge/science/laboratory/conda/envs/spacy/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/bridge/science/laboratory/conda/envs/spacy/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/bridge/science/laboratory/conda/envs/spacy/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/bridge/science/laboratory/conda/envs/spacy/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/bridge/science/laboratory/conda/envs/spacy/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/bridge/science/laboratory/conda/envs/spacy/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/bridge/science/laboratory/conda/envs/spacy/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig, BertTokenizer, BertModel, BertForMaskedLM\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(91768)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = \"/bridge/data/compositional_semantics\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = json.load(open('{}/ppa_train.json'.format(datadir)))\n",
    "#train_data = json.load(open(os.path.join([datadir,'ppa-hugging-face-train.json'])))\n",
    "#test_data = json.load(open(os.path.join([datadir,'ppa-hugging-face-test.json'])))\n",
    "test_data = json.load(open('{}/ppa_test.json'.format(datadir)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_huggingface_instances(model,tokenizer,\n",
    "                                   dataset,\n",
    "                                   orig_tokenizer=None,\n",
    "                                   max_length=128,\n",
    "                                   pad_to_max_length=True,\n",
    "                                   use_cuda=False):\n",
    "    sents_all = [instance['sentence_text'] for instance in dataset]\n",
    "    if use_cuda:\n",
    "        model.to('cuda')\n",
    "    retuple = lambda word_attr : (word_attr['text'],\n",
    "                                  word_attr['source'],\n",
    "                                  word_attr['pos_tag'],\n",
    "                                  word_attr['lemma'],\n",
    "                                  word_attr['trail_space'])\n",
    "    for instance in dataset:\n",
    "        annotated4tpl = (retuple(instance['V']),\n",
    "                         retuple(instance['N']),\n",
    "                         retuple(instance['P']),\n",
    "                         retuple(instance['N2']))\n",
    "        sent = instance['sentence_text']\n",
    "        label = instance['label']\n",
    "        if 'tokenized_sentence' in instance:\n",
    "            orig_tokens = [t[0] for t in instance['tokenized_sentence']]\n",
    "        elif orig_tokenizer is not None:\n",
    "            orig_tokens = [t.text for t in orig_tokenizer(sent)]\n",
    "        else:  \n",
    "            orig_tokens = sent\n",
    "        bert_tokens = [\"[CLS]\"]\n",
    "        orig_token_indexes = [int(tpl[1].split('.')[-1])-1 for tpl in annotated4tpl]\n",
    "        orig_4tpl_tokens = [orig_tokens[i] for i in orig_token_indexes]\n",
    "        orig_bert_token_indexes = []\n",
    "        word_pieces_array = []\n",
    "        for orig_token in orig_tokens:\n",
    "            #orig_to_tok_map.append(len(bert_tokens))\n",
    "            orig_bert_token_indexes.append(len(bert_tokens))\n",
    "            word_pieces = tokenizer.tokenize(orig_token)\n",
    "            word_pieces_array.append(word_pieces)\n",
    "            bert_tokens.extend(word_pieces)\n",
    "        bert_tokens.append(\"[SEP]\")\n",
    "        indexed_tokens = tokenizer.convert_tokens_to_ids(bert_tokens)\n",
    "        tokens_tensor = tokenizer.encode(indexed_tokens,\n",
    "                                         max_length=max_length,\n",
    "                                         pad_to_max_length=pad_to_max_length,\n",
    "                                         return_tensors='pt')\n",
    "        if use_cuda:\n",
    "            tokens_tensor = tokens_tensor.to('cuda')\n",
    "        x = []\n",
    "        y = []\n",
    "        with torch.no_grad():\n",
    "            # When output_hidden_states = True,\n",
    "            # the hidden states are output in the third value\n",
    "            # in the tuple returned from the model.\n",
    "            # That value is itself a tuple of the embedding matrix and\n",
    "            # hidden layers, 1-N (where N is the number of hidden layers)\n",
    "            # We want the last 4 layers of 24, which will be found in\n",
    "            # elements 21-24 of the second return tuple (embedding matrix is\n",
    "            # element 0).\n",
    "            hidden_layers = model(tokens_tensor)[2][21:25]\n",
    "            for orig_token_idx in orig_token_indexes:\n",
    "                word_pieces = word_pieces_array[orig_token_idx]\n",
    "                num_word_pieces = len(word_pieces)\n",
    "                # If token >1 piece, use layers from word pieces (4 total)\n",
    "                # 4th-from-top layer from first piece...\n",
    "                # top layer from 4th (or last) piece\n",
    "\n",
    "                layeridx = 3\n",
    "                wpi = 0\n",
    "                token_layers_values = []        \n",
    "                orig_bert_token_index = orig_bert_token_indexes[orig_token_idx]\n",
    "                while layeridx>=0:\n",
    "                    #tli = 0\n",
    "                    tli=layeridx\n",
    "                    #tli = 3-layeridx\n",
    "                    token_layer_values = hidden_layers[tli][0,orig_bert_token_index+wpi]\n",
    "                    #token_layers_values.extend([token_layer_values])\n",
    "                    #y.extend(token_layer_values)\n",
    "                    x.extend(token_layer_values)\n",
    "                    layeridx-=1\n",
    "                    if wpi<(num_word_pieces-1):\n",
    "                        wpi+=1\n",
    "                #flattened_layers = [xi for layer in token_layers_values for xi in layer]\n",
    "                #x.extend(flattened_layers)\n",
    "        #assert np.array_equal(x,y)\n",
    "        #print(\"{} => {}\".format(orig_4tpl_tokens,orig_4tpl_pieces))\n",
    "        yield x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train = [instance['label'] for instance in train_data]\n",
    "labels_test = [instance['label'] for instance in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_config = BertConfig.from_pretrained(\"bert-large-uncased-whole-word-masking\")\n",
    "bert_config.output_hidden_states=True\n",
    "\n",
    "bert_model = BertModel.from_pretrained(\"bert-large-uncased-whole-word-masking\",config=bert_config)\n",
    "bert_model.eval()\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking',config=bert_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.array([x for x in generate_huggingface_instances(\n",
    "    bert_model,bert_tokenizer,train_data,use_cuda=True)])\n",
    "test = np.array([x for x in generate_huggingface_instances(\n",
    "    bert_model,bert_tokenizer,test_data,use_cuda=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=100.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma=0.0001, kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=91768, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfhf = svm.SVC(gamma=0.0001, C=100., random_state=91768)\n",
    "\n",
    "clfhf.fit(train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6134147542598247"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_test_hf = clfhf.predict(test)\n",
    "kappa(labels_test, labels_test_hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.90909091, 0.68656716, 0.5       ])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(labels_test, labels_test_hf, labels=['N','V','O'], average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
