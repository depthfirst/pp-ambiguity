{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resolving Ambiguity in Prepositional Phrase Attachment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows results of predicting prepositional phrase attachments across a subset of the NLVR2 dataset which has been annotated. \n",
    "\n",
    "The first group of models are trained from the output the large uncased model from BERT with whole word masking. \n",
    "This model was subsequently converted to PyTorch/HuggingFace via command-line. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blah blah blah about prepositional phrase attachments... \n",
    "\n",
    "Blah blah blah some interesting examples. \n",
    "\n",
    "Blah blah blah about NLVR2 paper and dataset\n",
    "\n",
    "Some stuff about this dataset and how it was collected\n",
    "and how it was annotated\n",
    "\n",
    "What this notebook shows... \n",
    "\n",
    "(my sig)\n",
    "\n",
    "Prelims\n",
    "Imports\n",
    "outline/toc\n",
    "Background\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda create -n python=3.7 ...\n",
    "# pip install transformers... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bridge/science/laboratory/conda/envs/spacy/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/bridge/science/laboratory/conda/envs/spacy/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/bridge/science/laboratory/conda/envs/spacy/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/bridge/science/laboratory/conda/envs/spacy/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/bridge/science/laboratory/conda/envs/spacy/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/bridge/science/laboratory/conda/envs/spacy/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/bridge/science/laboratory/conda/envs/spacy/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/bridge/science/laboratory/conda/envs/spacy/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/bridge/science/laboratory/conda/envs/spacy/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/bridge/science/laboratory/conda/envs/spacy/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/bridge/science/laboratory/conda/envs/spacy/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/bridge/science/laboratory/conda/envs/spacy/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import torch\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import cohen_kappa_score as kappa\n",
    "from itertools import groupby\n",
    "\n",
    "from sklearn import svm\n",
    "from collections import Counter\n",
    "\n",
    "sys.path.append('/bridge/science/AI/nlp/bert')\n",
    "from notebook_source import load_text_file, load_xml_files, generate_tuples\n",
    "from notebook_source import load_folia_xml\n",
    "from notebook_source import find_sentence_from_file, find_sentence_from_word_id\n",
    "from notebook_source import generate_annotated4tpls, generate_sentences_from_4tpls\n",
    "from notebook_source import generate_google_instances, generate_huggingface_instances\n",
    "#import tokenization\n",
    "\n",
    "from transformers import BertConfig, BertTokenizer, BertModel, BertForMaskedLM\n",
    "from sklearn.neural_network import MLPClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(91768)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "anndir = \"/bridge/data/compositional_semantics/folia/jblackmore/done\"\n",
    "spacydir = \"/bridge/data/compositional_semantics/folia/dev\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents, generator = load_folia_xml(anndir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated4tpls = []\n",
    "tdeps = {}\n",
    "for t,dep in generator():\n",
    "    tdeps[t[2]] = dep\n",
    "    annotated4tpls.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "631"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tdeps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_sents, spacy_gen = load_folia_xml(spacydir)\n",
    "sdeps = {}\n",
    "spacy4tpls = []\n",
    "for spacy_tpl,sdep in spacy_gen():\n",
    "    sprep = spacy_tpl[2]\n",
    "    sdeps[sprep] = sdep\n",
    "    spacy4tpls.append(spacy_tpl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "930"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sdeps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "631"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(annotated4tpls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "930"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(spacy4tpls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated4tpls = [a4tpl for i,a4tpl in enumerate(annotated4tpls) if a4tpl[2] in sdeps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('flower', 'nlvr2_dev_002.text.s.24.w.6', 'NN', 'flower', True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdeps[annotated4tpls[21][2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_attachment_class = lambda tpl, deps : \\\n",
    "    '!' if tpl[2] not in deps else \\\n",
    "    'V' if deps[tpl[2]]==tpl[0] else \\\n",
    "    'N' if deps[tpl[2]]==tpl[1] else \\\n",
    "    'O'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [prep_attachment_class(tpl,tdeps) for tpl in annotated4tpls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'N': 442, 'V': 140, 'O': 47})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_preds = [prep_attachment_class(tpl,sdeps) for tpl in annotated4tpls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.275600163537006"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kappa(spacy_preds, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'V': 94, 'N': 465, 'O': 70})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(spacy_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "629"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(annotated4tpls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "930"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(spacy4tpls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>361</td>\n",
       "      <td>50</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>95</td>\n",
       "      <td>37</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1   2\n",
       "0  361  50  31\n",
       "1   95  37   8\n",
       "2    9   7  31"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(confusion_matrix(labels, spacy_preds, labels=['N','V','O']), index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sents_all = list(generate_sentences_from_4tpls(annotated4tpls,sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from notebook_source import stext\n",
    "#sents_all=[stext(find_sentence_from_word_id(t4tpl[0][1],sents)) for t4tpl in annotated4tpls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(sents_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write all sentences with annotations to disk for further\n",
    "# processing with BERT models. \n",
    "\n",
    "#with open(os.path.join(bert_datadir,'sents_all.txt'),'w') as allout:\n",
    "#    for s in sents_all:\n",
    "#        allout.write(s)\n",
    "#        allout.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... Wait for features from BERT ...\n",
    "#export BERT_BASE_DIR=/bridge/science/AI/nlp/corpora/BERT/wwm_uncased_L-24_H-1024_A-16\n",
    "#python extract_features.py --input_file=/bridge/science/AI/nlp/data/compositional_semantics/BERT/sents_all.txt --output_file=/bridge/science/AI/nlp/data/compositional_semantics/BERT/sents_all_wwmu_output.jsonl --vocab_file=$BERT_BASE_DIR/vocab.txt --bert_config_file=$BERT_BASE_DIR/bert_config.json --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt --layers=-1,-2,-3,-4 --max_seq_length=128 --batch_size=8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = np.array(X_list)\n",
    "test_size = int(len(annotated4tpls)/4)\n",
    "randidx = list(range(len(annotated4tpls)))\n",
    "np.random.shuffle(randidx)\n",
    "trainidx = randidx[test_size:]\n",
    "testidx = randidx[:test_size]\n",
    "labels_train = [labels[i] for i in trainidx]\n",
    "labels_test = [labels[i] for i in testidx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_all = list(generate_sentences_from_4tpls(annotated4tpls,sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert the same BERT model to work with huggingface with a command-line based converter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we're going to load the same BERT model through the huggingface\n",
    "transformers API. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to stack these in such a way that layers 4-3-2-1 appear for each of the 4 words, selected across word pieces. \n",
    "\n",
    "Ex: <br>\n",
    "Mary ate noodles with chopsticks. <br>\n",
    "Mary ate noodles with curry. <br>\n",
    "\n",
    "4-tuple (VNPN): ate, noodles, with, (chopsticks/curry)\n",
    "\n",
    "The BERT tokenizer may break up words, so it's possible to see something like \n",
    "ate,noodl#es, with, chop#sticks/cur#ry\n",
    "We take the 4 layers of up to 4 pieces of each word, starting with the \n",
    "4th layer of the first piece, then the\n",
    "3rd layer of the second/last piece, ...\n",
    "top layer of the fourth/last piece, \n",
    "So we'll have 16 piece-layers for each attachment instance. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=[]\n",
    "test_data=[]\n",
    "instance_4tpl_labels = [\"V\",\"N\",\"P\",\"N2\"]\n",
    "word_attribute_labels = [\"text\",\"source\",\"pos_tag\",\"lemma\",\"trail_space\"]\n",
    "for i,(ann4tpl,senttext,label) in enumerate(zip(annotated4tpls,sents_all,labels)):\n",
    "    new_instance = {\"sentence_text\": senttext, \"label\": label}\n",
    "    for word_label,token_tpl in zip(instance_4tpl_labels,ann4tpl):\n",
    "        word_attributes = {}\n",
    "        for word_attr_label,word_attr_value in zip(word_attribute_labels,token_tpl):\n",
    "            word_attributes[word_attr_label] = word_attr_value\n",
    "        new_instance[word_label] = word_attributes\n",
    "    if i in trainidx:\n",
    "        train_data.append(new_instance)\n",
    "    elif i in testidx:\n",
    "        test_data.append(new_instance)    \n",
    "    else:\n",
    "        raise ValueError(\"{} is out of bounds of dataset\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=[]\n",
    "test_d ata=[]\n",
    "instance_4tpl_labels = [\"V\",\"N\",\"P\",\"N2\"]\n",
    "word_attribute_labels = [\"text\",\"source\",\"pos_tag\",\"lemma\",\"trail_space\"]\n",
    "for i,(ann4tpl,senttext,label) in enumerate(zip(annotated4tpls,sents_all,labels)):\n",
    "    new_instance = {\"sentence_text\": senttext, \"label\": label}\n",
    "    for word_label,token_tpl in zip(instance_4tpl_labels,ann4tpl):\n",
    "        word_attributes = {}\n",
    "        for word_attr_label,word_attr_value in zip(word_attribute_labels,token_tpl):\n",
    "            word_attributes[word_attr_label] = word_attr_value\n",
    "        new_instance[word_label] = word_attributes\n",
    "    if i in trainidx:\n",
    "        train_data.append(new_instance)\n",
    "    elif i in testidx:\n",
    "        test_data.append(new_instance)    \n",
    "    else:\n",
    "        raise ValueError(\"{} is out of bounds of dataset\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_sents(dataset,sents):\n",
    "    new_dataset = []\n",
    "    for instance in dataset:\n",
    "        source = instance['V']['source']\n",
    "        foundit=False\n",
    "        for sent in sents:\n",
    "            sent_token_sources = [tok[1] for tok in sent]\n",
    "            if source in sent_token_sources:\n",
    "                new_dataset.append(sent)\n",
    "                foundit=True\n",
    "                continue\n",
    "        if not foundit:\n",
    "            raise ValueError(\"Couldn't find it: {}\".format(source))\n",
    "    return new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = \"/bridge/data/compositional_semantics\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(train_data,open('{}/ppa-hugging-face-train.json'.format(datadir),'w'),indent=4)\n",
    "json.dump(test_data,open('{}/ppa-hugging-face-test.json'.format(datadir),'w'),indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence_text': 'There is broccoli on a towel.',\n",
       " 'label': 'V',\n",
       " 'V': {'text': 'is',\n",
       "  'source': 'nlvr2_dev_019.text.s.2.w.2',\n",
       "  'pos_tag': 'VBZ',\n",
       "  'lemma': 'be',\n",
       "  'trail_space': True},\n",
       " 'N': {'text': 'broccoli',\n",
       "  'source': 'nlvr2_dev_019.text.s.2.w.3',\n",
       "  'pos_tag': 'NN',\n",
       "  'lemma': 'broccoli',\n",
       "  'trail_space': True},\n",
       " 'P': {'text': 'on',\n",
       "  'source': 'nlvr2_dev_019.text.s.2.w.4',\n",
       "  'pos_tag': 'IN',\n",
       "  'lemma': 'on',\n",
       "  'trail_space': True},\n",
       " 'N2': {'text': 'towel',\n",
       "  'source': 'nlvr2_dev_019.text.s.2.w.6',\n",
       "  'pos_tag': 'NN',\n",
       "  'lemma': 'towel',\n",
       "  'trail_space': False}}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[212]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sents = align_sents(train_data, sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sents = align_sents(test_data, sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zip_dataset_with_sents(dataset, sents_for_dataset):\n",
    "    zipped = []\n",
    "    for instance,sent in zip(dataset,sents_for_dataset):\n",
    "        instance['tokenized_sentence'] = sent\n",
    "        zipped.append(instance)\n",
    "    return zipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {}\n",
    "dataset['train'] = zip_dataset_with_sents(train_data, train_sents)\n",
    "dataset['test'] = zip_dataset_with_sents(test_data, test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(dataset['train'],open('{}/ppa_train.json'.format(datadir),'w'),indent=4)\n",
    "json.dump(dataset['test'],open('{}/ppa_test.json'.format(datadir),'w'),indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
